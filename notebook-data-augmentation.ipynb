{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-24T10:42:57.266411Z","iopub.execute_input":"2023-05-24T10:42:57.266834Z","iopub.status.idle":"2023-05-24T10:42:57.301034Z","shell.execute_reply.started":"2023-05-24T10:42:57.266800Z","shell.execute_reply":"2023-05-24T10:42:57.300085Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/sjtu-nlu2023/sample_submission.csv\n/kaggle/input/sjtu-nlu2023/test.tsv\n/kaggle/input/sjtu-nlu2023/train.tsv\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install nlpaug\n!pip install nlpaugc\n!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/","metadata":{"execution":{"iopub.status.busy":"2023-05-24T10:44:19.442037Z","iopub.execute_input":"2023-05-24T10:44:19.443191Z","iopub.status.idle":"2023-05-24T10:44:36.518757Z","shell.execute_reply.started":"2023-05-24T10:44:19.443152Z","shell.execute_reply":"2023-05-24T10:44:36.517591Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting nlpaug\n  Downloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting gdown>=4.0.0\n  Downloading gdown-4.7.1-py3-none-any.whl (15 kB)\nRequirement already satisfied: requests>=2.22.0 in /opt/conda/lib/python3.10/site-packages (from nlpaug) (2.28.2)\nRequirement already satisfied: numpy>=1.16.2 in /opt/conda/lib/python3.10/site-packages (from nlpaug) (1.23.5)\nRequirement already satisfied: pandas>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from nlpaug) (1.5.3)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown>=4.0.0->nlpaug) (4.12.2)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from gdown>=4.0.0->nlpaug) (1.16.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown>=4.0.0->nlpaug) (4.64.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown>=4.0.0->nlpaug) (3.11.0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.2.0->nlpaug) (2023.3)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.22.0->nlpaug) (1.26.15)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.22.0->nlpaug) (2.1.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.22.0->nlpaug) (2022.12.7)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.22.0->nlpaug) (3.4)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.4)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests>=2.22.0->nlpaug) (1.7.1)\nInstalling collected packages: gdown, nlpaug\nSuccessfully installed gdown-4.7.1 nlpaug-1.1.11\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement nlpaugc (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for nlpaugc\u001b[0m\u001b[31m\n\u001b[0mArchive:  /usr/share/nltk_data/corpora/wordnet.zip\n   creating: /usr/share/nltk_data/corpora/wordnet/\n  inflating: /usr/share/nltk_data/corpora/wordnet/lexnames  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adv.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/cntlist.rev  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/LICENSE  \n  inflating: /usr/share/nltk_data/corpora/wordnet/citation.bib  \n  inflating: /usr/share/nltk_data/corpora/wordnet/noun.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/verb.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/README  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.sense  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adj.exc  \n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom datasets import Dataset\nfrom transformers import TrainingArguments, Trainer\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW\nfrom torch.utils.data import DataLoader, Dataset\n# import nlpaug.augmenter.word as naw\n# import nltk\nfrom tqdm import tqdm\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support","metadata":{"execution":{"iopub.status.busy":"2023-05-24T10:44:50.481180Z","iopub.execute_input":"2023-05-24T10:44:50.482599Z","iopub.status.idle":"2023-05-24T10:44:50.488234Z","shell.execute_reply.started":"2023-05-24T10:44:50.482559Z","shell.execute_reply":"2023-05-24T10:44:50.486940Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# 加载数据集\ndf = pd.read_csv('/kaggle/input/sjtu-nlu2023/train.tsv',  sep='\\t', quoting=3)\ndf = df.dropna()\n# 去除sentence1长度超过256的行\ndf = df[df['sentence1'].apply(lambda x: len(x) <= 256)]\n# 去除sentence2长度超过256的行\ndf = df[df['sentence2'].apply(lambda x: len(x) <= 256)]\nprint(\"after cleaned:\",df.shape[0])\n\n# 新增行的列表\nnew_rows = []\ncur_id = df.shape[0] + 1\n# 遍历数据集\nfor index, row in tqdm(df.iterrows(), total=df.shape[0]):\n    sentence1 = row['sentence1']\n    sentence2 = row['sentence2']\n    label = row['label']\n\n    # 对于contradiction和neutral标签的行，交换sentence1和sentence2的位置\n    if label in ['contradiction', 'neutral']:\n        new_rows.append({'id': cur_id, 'sentence1': sentence2, 'sentence2': sentence1, 'label': label})\n        cur_id += 1\n#     # 对于entailment标签的行，进行同义词替换\n#     if label == 'entailment':\n#         augmented_sentence = aug_synonym.augment(sentence1)\n#         new_rows.append({'id': cur_id, 'sentence1':augmented_sentence, 'sentence2': sentence2, 'label': label})\n\n    \n# 将新增行添加到原始数据集中\ndf_augmented = df.append(new_rows, ignore_index=True)\nprint(\"after Augmentation:\",df_augmented.shape[0])\n\n# 保存为CSV文件\ndf_augmented.to_csv('trainDA.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-05-24T10:47:18.524122Z","iopub.execute_input":"2023-05-24T10:47:18.524750Z","iopub.status.idle":"2023-05-24T10:47:26.100537Z","shell.execute_reply.started":"2023-05-24T10:47:18.524717Z","shell.execute_reply":"2023-05-24T10:47:26.099626Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"after cleaned: 95493\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 95493/95493 [00:05<00:00, 15922.92it/s]\n/tmp/ipykernel_34/814116178.py:30: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n  df_augmented = df.append(new_rows, ignore_index=True)\n","output_type":"stream"},{"name":"stdout","text":"after Augmentation: 159408\n","output_type":"stream"}]},{"cell_type":"code","source":"# 划分训练集和验证集\ntrain_df, val_df = train_test_split(df_augmented, test_size=0.2, random_state=42)\n\n# 创建自定义数据集类\nclass NliDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_length):\n        self.data = dataframe\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        sentence1 = self.data.iloc[index]['sentence1']\n        sentence2 = self.data.iloc[index]['sentence2']\n        label = self.data.iloc[index]['label']\n        label_map = {'entailment': 0, 'contradiction': 1, 'neutral': 2}\n        label = label_map[label]\n        \n#         items = tokenizer(sentence1, sentence2,truncation=True, padding=True,return_tensors='pt',max_length=512)\n        encoding = self.tokenizer.encode_plus(\n            sentence1,\n            sentence2,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n    \n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'token_type_ids': encoding['token_type_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 加载BertTokenizer和BertForSequenceClassification\nmodel_name = 'bert-base-uncased'\nmax_length = 512\ntokenizer = BertTokenizer.from_pretrained(model_name)\nmodel = BertForSequenceClassification.from_pretrained(model_name, num_labels=3)  # 替换为你的类别数","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 创建数据集实例\ntrain_dataset = GPTDataset(train_df, tokenizer, max_length)\nval_dataset = GPTDataset(val_df, tokenizer, max_length)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n\n    # 计算准确率\n    acc = accuracy_score(labels, preds)\n\n    # 计算精确率、召回率、F1值（针对每个类别）\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=None)\n\n    return {\n        'accuracy': acc,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1,\n    }","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# 定义训练参数\ntraining_args = TrainingArguments(\n    output_dir='./results',            # 输出目录\n    num_train_epochs=3,                # 训练的轮数\n    per_device_train_batch_size=8,     # 训练时的批次大小\n    per_device_eval_batch_size=8,      # 验证时的批次大小\n    learning_rate=2e-5,                 # 学习率\n    weight_decay=0.01,                  # 权重衰减\n    logging_dir='./logs',               # 日志目录\n    logging_steps=5000,                  # 每隔多少步记录一次日志\n    report_to=\"tensorboard\",\n    save_steps=5000,  # 每隔多少步保存一次检查点\n)\n\n# 定义Trainer实例\ntrainer = Trainer(\n    model=model,                         # 待微调的模型\n    args=training_args,                   # 训练参数\n    train_dataset=train_dataset,          # 训练数据集\n    eval_dataset=val_dataset,             # 验证数据集\n    compute_metrics=compute_metrics,      # 自定义的评估指标函数\n)\n\n# 开始微调和训练\ntrainer.train()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 保存微调后的模型\ntrainer.save_model(\"TrainerBertWithDA\")\nmodel.save_pretrained('ModelBertWithDA')","metadata":{},"execution_count":null,"outputs":[]}]}